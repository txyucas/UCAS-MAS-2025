{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96c141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd485a",
   "metadata": {},
   "source": [
    "## 1.定义算法\n",
    "### 1.1 建立Actor和Critic网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39da0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, n_states, n_actions, hidden_dim = 256):\t\n",
    "\t\t\"\"\" 初始化Actor网络，为全连接网络\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\t\tself.l1 = nn.Linear(n_states, hidden_dim)\n",
    "\t\tself.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\t\tself.l3 = nn.Linear(hidden_dim, n_actions)\n",
    "\t\n",
    "\tdef forward(self, state):\n",
    "\t\t\n",
    "\t\tx = F.relu(self.l1(state))\n",
    "\t\tx = F.relu(self.l2(x))\n",
    "\t\tx = torch.tanh(self.l3(x))\n",
    "\t\treturn x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, n_states, n_actions, hidden_dim = 256):\n",
    "\t\t\"\"\" 初始化Critic网络，为全连接网络\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\t\tself.l1 = nn.Linear(n_states + n_actions, 256)\n",
    "\t\tself.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\t\tself.l3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\t\tq = F.relu(self.l1(sa))\n",
    "\t\tq = F.relu(self.l2(q))\n",
    "\t\tq = self.l3(q)\n",
    "\t\treturn q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446652dc",
   "metadata": {},
   "source": [
    "### 1.2 定义经验回放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d1e69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferQue:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "    def push(self,transitions):\n",
    "        ''' 存储transition到经验回放中\n",
    "        '''\n",
    "        self.buffer.append(transitions)\n",
    "    def sample(self, batch_size: int, sequential: bool = False):\n",
    "        if batch_size > len(self.buffer): # 如果批量大小大于经验回放的容量，则取经验回放的容量\n",
    "            batch_size = len(self.buffer)\n",
    "        if sequential: # 顺序采样\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            batch = [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "            return zip(*batch)\n",
    "        else: # 随机采样\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "            return zip(*batch)\n",
    "    def clear(self):\n",
    "        ''' 清空经验回放\n",
    "        '''\n",
    "        self.buffer.clear()\n",
    "    def __len__(self):\n",
    "        ''' 返回当前存储的量\n",
    "        '''\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8121f6",
   "metadata": {},
   "source": [
    "### 1.3 TD3算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a89624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "\tdef __init__(self,cfg):\n",
    "\t\tsuper(TD3, self).__init__()\n",
    "\t\tself.gamma = cfg.gamma # 奖励的折扣因子\n",
    "\t\tself.actor_lr = cfg.actor_lr # actor学习率\n",
    "\t\tself.critic_lr = cfg.critic_lr # critic学习率\n",
    "\t\tself.policy_noise = cfg.policy_noise\n",
    "\t\tself.noise_clip = cfg.noise_clip\n",
    "\t\tself.expl_noise = cfg.expl_noise\n",
    "\t\tself.policy_freq = cfg.policy_freq\n",
    "\t\tself.batch_size =  cfg.batch_size \n",
    "\t\tself.tau = cfg.tau\n",
    "\t\tself.sample_count = 0\n",
    "\t\tself.policy_freq = cfg.policy_freq\n",
    "\t\tself.explore_steps = cfg.explore_steps\n",
    "\t\tself.device = torch.device(cfg.device)\n",
    "\t\tself.n_actions = cfg.n_actions\n",
    "\t\tself.action_space = cfg.action_space\n",
    "\t\tself.action_scale = torch.tensor((self.action_space.high - self.action_space.low)/2, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "\t\tself.action_bias = torch.tensor((self.action_space.high + self.action_space.low)/2, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "\t\tself.actor = Actor(cfg.n_states, cfg.n_actions, hidden_dim = cfg.actor_hidden_dim).to(self.device)\n",
    "\t\tself.actor_target = Actor(cfg.n_states, cfg.n_actions, hidden_dim = cfg.actor_hidden_dim).to(self.device)\n",
    "\t\tself.actor_target.load_state_dict(self.actor.state_dict()) # 复制参数到目标网络\n",
    "\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = self.actor_lr)\n",
    "\n",
    "\t\tself.critic_1 = Critic(cfg.n_states, cfg.n_actions, hidden_dim = cfg.critic_hidden_dim).to(self.device)\n",
    "\t\tself.critic_2 = Critic(cfg.n_states, cfg.n_actions, hidden_dim = cfg.critic_hidden_dim).to(self.device)\n",
    "\t\tself.critic_1_target = Critic(cfg.n_states, cfg.n_actions, hidden_dim = cfg.critic_hidden_dim).to(self.device)\n",
    "\t\tself.critic_2_target = Critic(cfg.n_states, cfg.n_actions, hidden_dim = cfg.critic_hidden_dim).to(self.device)\n",
    "\t\tself.critic_1_target.load_state_dict(self.critic_1.state_dict()) # 复制参数到目标网络\n",
    "\t\tself.critic_2_target.load_state_dict(self.critic_2.state_dict()) # 复制参数到目标网络\n",
    "\t\t\n",
    "\t\tself.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(), lr = self.critic_lr)\n",
    "\t\tself.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(), lr = self.critic_lr)\n",
    "\t\tself.memory = ReplayBufferQue(cfg.buffer_size)\n",
    "\t\t# self.memory = ReplayBuffer(n_states, n_actions)\n",
    "\n",
    "\tdef sample_action(self, state):\n",
    "        \n",
    "        #采样动作\n",
    "        \n",
    "\t\tself.sample_count += 1\n",
    "\t\tif self.sample_count < self.explore_steps:\n",
    "\t\t\treturn self.action_space.sample()\n",
    "\t\telse:\n",
    "\t\t\tstate = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "\t\t\taction = self.actor(state)\n",
    "\t\t\taction = self.action_scale * action + self.action_bias\n",
    "\t\t\taction = action.detach().cpu().numpy()[0]\n",
    "\t\t\taction_noise = np.random.normal(0, self.action_scale.cpu().numpy()[0] * self.expl_noise, size=self.n_actions)\n",
    "\t\t\taction = (action + action_noise).clip(self.action_space.low, self.action_space.high)\n",
    "\t\t\treturn action\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef predict_action(self, state):\n",
    "        #采样动作\n",
    "\t\tstate = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "\t\taction = self.actor(state)\n",
    "\t\taction = self.action_scale * action + self.action_bias # 对actor计算的动作分布放缩\n",
    "\t\treturn action.detach().cpu().numpy()[0]\n",
    "\n",
    "\tdef update(self):\n",
    "\t\t# if len(self.memory) < self.batch_size:\n",
    "\t\t# \treturn\n",
    "\t\tif len(self.memory) < self.explore_steps: # 当经验回放中不满足一个批量时，不更新策略\n",
    "\t\t\treturn\n",
    "\t\tstate, action, reward, next_state, done = self.memory.sample(self.batch_size) # 从经验回放中随机采样一个批量的转移(transition)\n",
    "         # 将数据转换为tensor\n",
    "\t\tstate = torch.tensor(np.array(state), device=self.device, dtype=torch.float32)\n",
    "\t\taction = torch.tensor(np.array(action), device=self.device, dtype=torch.float32)\n",
    "\t\tnext_state = torch.tensor(np.array(next_state), device=self.device, dtype=torch.float32)\n",
    "\t\treward = torch.tensor(reward, device=self.device, dtype=torch.float32).unsqueeze(1)\n",
    "\t\tdone = torch.tensor(done, device=self.device, dtype=torch.float32).unsqueeze(1)\n",
    "\t\t# update critic\n",
    "\t\tnoise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip) # 构造加入目标动作的噪声\n",
    "        # 计算加入了噪声的目标动作\n",
    "\t\tnext_action = (self.actor_target(next_state) + noise).clamp(-self.action_scale+self.action_bias, self.action_scale+self.action_bias)\n",
    "        # 计算两个critic网络对t+1时刻的状态动作对的评分，并选取更小值来计算目标q值\n",
    "\t\ttarget_q1, target_q2 = self.critic_1_target(next_state, next_action).detach(), self.critic_2_target(next_state, next_action).detach()\n",
    "\t\ttarget_q = torch.min(target_q1, target_q2)\n",
    "\t\ttarget_q = reward + self.gamma * target_q * (1 - done)\n",
    "        # 计算两个critic网络对t时刻的状态动作对的评分\n",
    "\t\tcurrent_q1, current_q2 = self.critic_1(state, action), self.critic_2(state, action)\n",
    "        # 计算均方根损失\n",
    "\t\tcritic_1_loss = F.mse_loss(current_q1, target_q)\n",
    "\t\tcritic_2_loss = F.mse_loss(current_q2, target_q)\n",
    "\t\tself.critic_1_optimizer.zero_grad()\n",
    "\t\tcritic_1_loss.backward()\n",
    "\t\tself.critic_1_optimizer.step()\n",
    "\t\tself.critic_2_optimizer.zero_grad()\n",
    "\t\tcritic_2_loss.backward()\n",
    "\t\tself.critic_2_optimizer.step()\n",
    "\t\tif self.sample_count % self.policy_freq == 0:\n",
    "            # 延迟策略更新，actor的更新频率低于critic\n",
    "\t\t\tactor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\tactor_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "            #目标网络软更新\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\t\t\tfor param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\t\t\tfor param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5335d",
   "metadata": {},
   "source": [
    "## 2.模型训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a73580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, env, agent):\n",
    "    print(\"开始训练！\")\n",
    "    rewards = [] # 记录所有回合的奖励\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        state = env.reset() # 重置环境，返回初始状态\n",
    "        ep_reward = 0 # 记录一回合内的奖励\n",
    "        for i_step in range(cfg.max_steps):\n",
    "            action = agent.sample_action(state)  # 抽样动作\n",
    "            next_state, reward, terminated, info = env.step(action)  # 更新环境，返回transitions\n",
    "            agent.memory.push((state, action, reward,\n",
    "                            next_state, terminated))  # 保存transition\n",
    "            agent.update()   # 更新智能体\n",
    "            state = next_state  # 更新下一个状态\n",
    "            ep_reward += reward  # 累加奖励\n",
    "            if terminated:\n",
    "                break\n",
    "        if (i_ep+1)%10 == 0:\n",
    "            print(f\"回合：{i_ep+1}/{cfg.train_eps}，奖励：{ep_reward:.2f}\")\n",
    "        rewards.append(ep_reward)\n",
    "    print(\"完成训练！\")\n",
    "    return {'rewards':rewards}\n",
    "def test(cfg, env, agent):\n",
    "    print(\"开始测试！\")\n",
    "    rewards = [] # 记录所有回合的奖励\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        state = env.reset() # 重置环境，返回初始状态\n",
    "        ep_reward = 0 # 记录一回合内的奖励\n",
    "        for i_step in range(cfg.max_steps):\n",
    "            action = agent.sample_action(state)  # 抽样动作\n",
    "            next_state, reward, terminated, info = env.step(action)  # 更新环境，返回transitions\n",
    "            state = next_state  # 更新下一个状态\n",
    "            ep_reward += reward  # 累加奖励\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"回合：{i_ep+1}/{cfg.test_eps}，奖励：{ep_reward:.2f}\")\n",
    "    print(\"完成测试！\")\n",
    "    return {'rewards':rewards}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e3b7b",
   "metadata": {},
   "source": [
    "## 3.定义环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b2350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "def all_seed(env,seed = 1):\n",
    "    ''' 万能的seed函数\n",
    "    '''\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # config for CPU\n",
    "    torch.cuda.manual_seed(seed) # config for GPU\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # config for python scripts\n",
    "    # config for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "def env_agent_config(cfg):\n",
    "    env = gym.make(cfg.env_name) # 创建环境\n",
    "    all_seed(env,seed=1)\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.shape[0]\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}\")\n",
    "    # 更新n_states和n_actions到cfg参数中\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions) \n",
    "    setattr(cfg, 'action_space', env.action_space) \n",
    "    models = {\"actor\":Actor(n_states,n_actions,hidden_dim=cfg.actor_hidden_dim),\"critic\":Critic(n_states,n_actions,hidden_dim=cfg.critic_hidden_dim)}\n",
    "    memory = ReplayBufferQue(cfg.buffer_size) # 创建经验池\n",
    "    agent = TD3(cfg)\n",
    "    return env,agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dedbaf",
   "metadata": {},
   "source": [
    "## 4.设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d5035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.algo_name = 'TD3' # 算法名称\n",
    "        self.env_name = 'Pendulum-v1' # 环境名称\n",
    "        self.device = \"cuda\" # 使用设备\n",
    "        self.explore_steps = 1000 # 探索步数\n",
    "        self.train_eps = 1000 # 训练迭代次数\n",
    "        self.test_eps = 20 # 测试迭代次数\n",
    "        self.eval_eps = 10 # 评估迭代次数\n",
    "        self.eval_per_episode = 5 # 每隔几代评估\n",
    "        self.max_steps = 200 # 每次迭代最大时间步\n",
    "        self.policy_freq = 2  # 策略网络更新频率\n",
    "        self.actor_lr = 1e-3 \n",
    "        self.critic_lr = 1e-3 \n",
    "        self.actor_hidden_dim = 256 # actor网络隐藏层维度\n",
    "        self.critic_hidden_dim = 256 # critic网络隐藏层维度\n",
    "        self.gamma = 0.99 \n",
    "        self.tau = 0.005 # 目标网络软更新系数\n",
    "        self.policy_noise = 0.2 # 加入策略网络的噪声\n",
    "        self.expl_noise = 0.1 # 高斯噪声标准差\n",
    "        self.noise_clip = 0.5 # 加入策略网络噪声范围\n",
    "        self.batch_size = 100 # 训练批次大小\n",
    "        self.buffer_size = 1000000 # 经验回放池大小\n",
    "def smooth(data, weight=0.9):  \n",
    "    '''用于平滑曲线，类似于Tensorboard中的smooth曲线\n",
    "    '''\n",
    "    last = data[0] \n",
    "    smoothed = []\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # 计算平滑值\n",
    "        smoothed.append(smoothed_val)                    \n",
    "        last = smoothed_val                                \n",
    "    return smoothed\n",
    "\n",
    "def plot_rewards(rewards,title=\"learning curve\"):\n",
    "    sns.set()\n",
    "    plt.figure()  # 创建一个图形实例，方便同时多画几个图\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.xlim(0, len(rewards), 10)  # 设置x轴的范围\n",
    "    plt.xlabel('epsiodes')\n",
    "    plt.plot(rewards, label='rewards')\n",
    "    plt.plot(smooth(rewards), label='smoothed')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c951326",
   "metadata": {},
   "source": [
    "## 5.开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d95733d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\86157\\.conda\\envs\\joyrl\\lib\\site-packages\\gym\\core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "c:\\Users\\86157\\.conda\\envs\\joyrl\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：3，动作空间维度：1\n",
      "开始训练！\n",
      "回合：10/1000，奖励：-1528.30\n",
      "回合：20/1000，奖励：-1514.86\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14040\\408702382.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_agent_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mres_dic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplot_rewards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rewards'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf\"training curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14040\\2708094640.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cfg, env, agent)\u001b[0m\n\u001b[0;32m     10\u001b[0m             agent.memory.push((state, action, reward,\n\u001b[0;32m     11\u001b[0m                             next_state, terminated))  # 保存transition\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 更新智能体\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m  \u001b[1;31m# 更新下一个状态\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mep_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m  \u001b[1;31m# 累加奖励\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14040\\2860317511.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                         \u001b[0mactor_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[1;31m#目标网络软更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_param\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86157\\.conda\\envs\\joyrl\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86157\\.conda\\envs\\joyrl\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86157\\.conda\\envs\\joyrl\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86157\\.conda\\envs\\joyrl\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 获取参数\n",
    "cfg = Config() \n",
    "# 训练\n",
    "env, agent = env_agent_config(cfg)\n",
    "res_dic = train(cfg, env, agent)\n",
    " \n",
    "plot_rewards(res_dic['rewards'], title=f\"training curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")  \n",
    "# 测试\n",
    "res_dic = test(cfg, env, agent)\n",
    "plot_rewards(res_dic['rewards'], title=f\"testing curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")  # 画出结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joyrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
