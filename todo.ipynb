{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO部分\n",
    "- sample_action\n",
    "  - 将reward缩小100倍，看看会不会减小梯度的剧烈变化\n",
    "  - 不对sigma进行截断\n",
    "  - 动作截断？\n",
    "- act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def act(self, obs, test_mode=False, exploration_scale=0.3):\n",
    "    \"\"\"\n",
    "    改进版动作采样函数，针对稀疏奖励和连续动作空间优化\n",
    "    参数：\n",
    "        obs: 环境观测字典，必须包含'agent_obs'键\n",
    "        test_mode: 测试模式开关，True时关闭探索噪声\n",
    "        exploration_scale: 初始探索强度系数(0.1~0.5)\n",
    "    返回：\n",
    "        list: 2D动作 [action_dim1, action_dim2]\n",
    "    \"\"\"\n",
    "    # 状态预处理\n",
    "    state = torch.tensor(obs['agent_obs'], dtype=torch.float).to(self.device)\n",
    "    state = state.unsqueeze(0)  # 添加batch维度 [1, ...]\n",
    "    \n",
    "    # 获取动作分布\n",
    "    with torch.no_grad():\n",
    "        mu, sigma, (self.actor_h, self.actor_c) = self.actor(\n",
    "            self.update_history_sequence(state),\n",
    "            self.actor_h, self.actor_c\n",
    "        )\n",
    "        \n",
    "        # 约束标准差范围并检查异常\n",
    "        sigma = torch.clamp(sigma, min=1e-3, max=1.0)\n",
    "        if torch.isnan(mu).any() or torch.isnan(sigma).any():\n",
    "            self.isnan = True\n",
    "            print(\"警告: 动作分布异常，使用安全动作\")\n",
    "            mu = torch.zeros_like(mu)\n",
    "            sigma = torch.ones_like(sigma) * 0.1\n",
    "        else:\n",
    "            self.isnan = False\n",
    "\n",
    "    # 稀疏奖励环境专用探索策略\n",
    "    if not test_mode:\n",
    "        # 自适应探索噪声（训练步数越多噪声越小）\n",
    "        decay_factor = max(0.1, 1 - self.sample_count / 2e6)  # 200万步衰减到10%\n",
    "        noise = exploration_scale * decay_factor * torch.randn_like(mu) * sigma\n",
    "        \n",
    "        # 对关键维度加强探索（示例：第2维度）\n",
    "        noise[:, 1] *= 1.5  # 第2维度的探索强度增加50%\n",
    "        \n",
    "        action = mu + noise\n",
    "    else:\n",
    "        action = mu  # 测试模式直接使用均值\n",
    "\n",
    "    # 动作截断（根据环境物理限制）\n",
    "    action = action.cpu().numpy()\n",
    "    action[0, 0] = np.clip(action[0, 0], -100, 200)  # 第一维限制\n",
    "    action[0, 1] = np.clip(action[0, 1], -30, 30)    # 第二维限制\n",
    "    \n",
    "    return action[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- update\n",
    "  - 不对ratio作截断"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
